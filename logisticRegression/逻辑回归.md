# 逻辑回归

## 什么是逻辑回归
要理解什么是逻辑回归，第一步，是要分别理解什么叫做回归

这里的回归函数也叫做**线性回归**

线性回归是用于连续变量预测的，类似于一元一次方程，二元一次方程等等

一般公式都类似于下面这样
> $ f(x)= w_1*x_1 + w_2*x_2 + ... + w_n*x_n + b$

回归模型可以用于预测房价，工资等等的连续变量

而**逻辑回归**则是一个分类器，即输入X后会输出一个离散的值

比如在使用逻辑回归做二分类器的时候，逻辑回归模型输出的就是 0 或 1 

逻辑回归的之所以有回归两个字，是因为逻辑回归其实就是从线性回归变过来的。唯一的变化是在线性回归外面包了一层函数，将线性回归的值压缩到( 0 , 1 )之间。当输出的值大于等于0.5的时候则判断为1, 当小于0.5的时候则判断为0
>上面是以二分类为例子，在多分类中可能就不是以0.5为临界值了


## 逻辑回归的原理
逻辑回归凭什么能够将(-∞, +∞)的值都压缩成(0, 1)呢

这个时候就要提到大名鼎鼎的sigmoid函数啦

### $ S(x) = \frac{1}{1 + e^{-x}}$
![来自百度图片](https://gss0.bdstatic.com/94o3dSag_xI4khGkpoWK1HF6hhy/baike/c0%3Dbaike80%2C5%2C5%2C80%2C26/sign=892a5cb4c55c10383073c690d378f876/c9fcc3cec3fdfc03f23fbf16d73f8794a5c226dc.jpg)

> 上面是sigmoid 函数图像 

sigmoid函数的值域为(0, 1)，无论输入的x有多大或者多小，都会被映射到(0, 1)之间

但是，为什么同样的输入X，经过线性回归和逻辑回归训练出来的差异会那么大呢？

其实关键在于训练数据的输出值上。线性回归的输出值是连续变量，而逻辑回归的输出值为离散值0/1

所以就会拟合出两种截然不同的参数。

线性回归为了尽量降低它拟合训练数据的误差，计算出来的直线（或者超平面）会尽量离所有的训练数据近，所以就会形成类似下面的图像

![线性回归](http://p1yxapae6.bkt.clouddn.com//ml/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92.png)

而逻辑回归则为了能够将正确的结果划分出来，会出现一条直线（或者超平面）将数据划分为两类。最终的图像会像下面这样

![逻辑回归](http://p1yxapae6.bkt.clouddn.com//ml/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92.png)




## 二项式逻辑回归模型
### $ P(y=1|X;θ) = g(θ^TX) = \frac{e^{-θ^TX}}{1 + e^{-θ^TX}} $
### $ P(y=0|X;θ) = g(θ^TX) = \frac{1}{1 + e^{-θ^TX}} $
二项式逻辑回归模型就是符合上面两个公式的条件概率分布。

## 模型参数的估计
模型的训练过程就是找到最合理的θ值让逻辑回归模型能够最好的区分出给定的数据。

统计学中经常使用极大似然估值来计算参数。
即找到一组参数使得在这组参数下，我们的数据概率（似然度）最大。
> 似然度(Likelihood)： 可以理解为在θ参数的条件下，输入X得到y的可能性

逻辑回归的似然度公式如下：
### $ L(θ) = \prod P(y|X;θ) = \prod g(θ^TX)^y(1 - g(θ^TX))^{1-y} $

对数似然度公式如下：
### $ l(θ) = \sum y*log(g(θ^T X)) + (1-y)*log(1-g(θ^TX)) $
> 对数似然度让乘法变成了加法，从而避免了浮点数下溢，同时也简化了求解过程

对数似然度也就是对数似然损失值。
而平均对数似然损失值即为：
### $ J(θ) = -\frac{1}{N} l(θ) $

而求最佳的θ只需要迭代计算最小化平均对数似然损失值就可以了啦～
> 当然损失函数不只有对数似然损失值，还有0-1损失,hinge损失等等

## 最小化损失函数
这个课题就不属于这篇博客的内容了

最常见的方案就是**梯度下降**，当然还有很多其他方案，在这里就不一一提及了
> 其实其他方案我也不太解，水平有限T^T见谅
> 
> 此乃本小弟的第一篇技术博客，望各位看官发现问题及时告知，轻拍。
> 
> 感谢～











