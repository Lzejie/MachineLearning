# 逻辑回归(未完成)

## 什么是逻辑回归
要理解什么是逻辑回归，第一步，是要分别理解什么叫做回归

这里的回归函数也叫做**线性回归**

线性回归是用于连续变量预测的，类似于一元一次方程，二元一次方程等等

一般公式都类似于下面这样
> $ f(x)= w_1*x_1 + w_2*x_2 + ... + w_n*x_n + b$

回归模型可以用于预测房价，工资等等的连续变量

而**逻辑回归**则是一个分类器，即输入X后会输出一个离散的值

比如在使用逻辑回归做二分类器的时候，逻辑回归模型输出的就是 0 或 1 

逻辑回归的之所以有回归两个字，是因为逻辑回归其实就是从线性回归变过来的。唯一的变化是在线性回归外面包了一层函数，将线性回归的值压缩到( 0 , 1 )之间。当输出的值大于等于0.5的时候则判断为1, 当小于0.5的时候则判断为0
>上面是以而分类为例子，在多分类中可能就不是以0.5为临界值了


## 逻辑回归的原理
逻辑回归凭什么能够将(-∞, +∞)的值都压缩成(0, 1)呢

这个时候就要提到大名鼎鼎的sigmoid函数啦

### $ S(x) = \frac{1}{1 + e^{-x}}$
![来自百度图片](https://gss0.bdstatic.com/94o3dSag_xI4khGkpoWK1HF6hhy/baike/c0%3Dbaike80%2C5%2C5%2C80%2C26/sign=892a5cb4c55c10383073c690d378f876/c9fcc3cec3fdfc03f23fbf16d73f8794a5c226dc.jpg)

> 上面是sigmoid 函数图像 

sigmoid函数的值域为(0, 1)，无论输入的x有多大或者多小，都会被映射到(0, 1)之间

但是，为什么同样的输入X，经过线性回归和逻辑回归训练出来的差异会那么大呢？

其实关键在于训练数据的输出值上。线性回归的输出值是连续变量，而逻辑回归的输出值为离散值0/1

所以就会拟合出两种截然不同的参数。

线性回归为了尽量降低它拟合训练数据的误差，计算出来的直线（或者超平面）会尽量离所有的训练数据近，所以就会形成类似下面的图像

![线性回归](http://p1yxapae6.bkt.clouddn.com//ml/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92.png)

而逻辑回归则为了能够将正确的结果划分出来，会出现一条直线（或者超平面）将数据划分为两类。最终的图像会像下面这样

![逻辑回归](http://p1yxapae6.bkt.clouddn.com//ml/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92.png)






## 推导过程
- 逻辑回归的推导
- 梯度下降的推导